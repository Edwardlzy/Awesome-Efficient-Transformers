# Awesome-Efficient-Transformers
This repo is a collection of awesome efficient transformers.

## Table of Contents
- Papers
- Blogs
- Datasets

## Papers
- Attention Is All You Need (NeurIPS 2017) [[paper](https://arxiv.org/abs/1706.03762)] [[code](https://github.com/tensorflow/tensor2tensor)]
- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (NAACL 2019) [[paper](https://arxiv.org/abs/1810.04805)]
- Language Models are Few-Shot Learners [[paper](https://arxiv.org/abs/2005.14165)]

### Low Rank Methods
- Rethinking Attention with Performers (ICLR 2021) [[paper](https://arxiv.org/abs/2009.14794)]
- Linformer: Self-Attention with Linear Complexity [[paper](https://arxiv.org/abs/2006.04768)] [[code](https://github.com/tatp22/linformer-pytorch)]
- Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (ICML 2020) [[paper](https://arxiv.org/abs/2006.16236)] [[code](https://linear-transformers.com)]
- Synthesizer: Rethinking Self-Attention in Transformer Models (ICML 2021) [[paper](https://arxiv.org/abs/2005.00743)]
 
### Memory Compression
- Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks (ICML 2019) [[paper](https://arxiv.org/abs/1810.00825)]
- Generating Wikipedia by Summarizing Long Sequences (ICLR 2018) [[paper](https://arxiv.org/abs/1801.10198)]
- Longformer: The Long-Document Transformer [[paper](https://arxiv.org/abs/2004.05150)] [[code](https://github.com/allenai/longformer)]
- ETC: Encoding Long and Structured Inputs in Transformers (EMNLP 2020) [[paper](https://arxiv.org/abs/2004.08483)]
- Big Bird: Transformers for Longer Sequences (NeurIPS 2020) [[paper](https://arxiv.org/abs/2007.14062)]
- Efficient Content-Based Sparse Attention with Routing Transformers (TACL 2020) [[paper](https://arxiv.org/abs/2003.05997)]
- Compressive Transformers for Long-Range Sequence Modelling [[paper](https://arxiv.org/abs/1911.05507)]

### Attention Matrix Patterns
- Blockwise Self-Attention for Long Document Understanding (Workshop at EMNLP 2020) [[paper](https://arxiv.org/abs/1911.02972)]
- Generating Long Sequences with Sparse Transformers [[paper](https://arxiv.org/abs/1904.10509)]
- Axial Attention in Multidimensional Transformers [[paper](https://arxiv.org/abs/1912.12180)]
- Image Transformer (ICML 2018) [[paper](https://arxiv.org/abs/1802.05751)]
- Sparse Sinkhorn Attention [[paper](https://arxiv.org/abs/2002.11296)]

### Other Efficient Transformers / BERT
